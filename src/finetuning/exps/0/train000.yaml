outf: 'finetune/train_0_000'
timelimit: -1
timescale: 1.3
logger:
  name: ''
  overwrite: 1
manager:
  ckpt_prefix: ''
seed: 0
debug: false
data:
  train:
    dataset:
      params:
        fold_group: 0
        two_class: 
          - AD_pos
          - NL_neg
        type: train
    dataloader:
      collate_fn: __collate__
      batch_size: 4
      num_workers: 4
      shuffle: true
      drop_last: 0
  val:
    dataset:
      params:
        fold_group: 0
        two_class: 
          - AD_pos
          - NL_neg
        type: val
    dataloader:
      collate_fn: __collate__
      batch_size: 4
      num_workers: 4
      drop_last: 0
      shuffle: true
  test:
    dataset:
      params:
        fold_group: 0
        two_class: 
          - AD_pos
          - NL_neg
        type: test
    dataloader:
      collate_fn: __collate__
      batch_size: 4
      num_workers: 4
      drop_last: 0
      shuffle: true
model:
  params:
    en_config:
      num_faces: 8192
      num_cls: 40
      cfg: 
        num_kernel: 64
        ConvSurface:
          num_samples_per_neighbor: 4
          rs_mode: 'Weighted'
          num_kernel: 64
        MeshBlock:
          blocks: [3, 4, 4]
      pool_rate: 4
      mask_percentage: 0.3
    de_config:
      in_channel: 1024
      bneck_size: 1024
      num_cls: 2
optim:
  name: 'Adam'
  params:
    lr: 1.0e-4
    betas: [0.9,0.999]
    eps: 1.0e-8
    weight_decay: 0.0
  scheduler:
    name: MultiStepLR
    params:
      milestones: [30, 60]
      gamma: 0.5
train:
  max_epoch: 50
  train_freq: 5
  save_at_train: true
